import os
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
from datetime import datetime

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '16'))  # æ˜¾å­˜ä¸å¤Ÿå¯è°ƒå°ï¼Œå¦‚ 2/4/8
LEARNING_RATE = 1e-5     # å­¦ä¹ ç‡
EPOCHS = 100             # æ€»è®­ç»ƒè½®æ•°(é¦–æ¬¡è®­ç»ƒç”¨)ï¼›å¾®è°ƒæ—¶ç”¨ EXTRA_EPOCHS æ§åˆ¶é¢å¤–è½®æ•°
DATA_DIR = os.getenv('DATA_DIR', './processed_data/26ms' if os.path.isdir('./processed_data/26ms') else './processed_data')  # æ•°æ®è·¯å¾„
SAVE_DIR = './checkpoints'     # æ¨¡å‹ä¿å­˜è·¯å¾„

# ================= å¾®è°ƒ(ä¸é‡è®­)é…ç½® =================
# åŠ¡å¿…ä»å·²æœ‰ checkpoint æ¢å¤æƒé‡ä¸ä¼˜åŒ–å™¨ï¼Œä¸è¦ä»å¤´å¼€å§‹è®­ç»ƒ
FINETUNE_FROM = os.getenv('FINETUNE_FROM', './checkpoints/resnet_epoch_100.pth')
EXTRA_EPOCHS = int(os.getenv('EXTRA_EPOCHS', '50'))  # å»ºè®® 30-50
RESUME_LATEST = int(os.getenv('RESUME_LATEST', '1')) # ä¸­æ–­åé»˜è®¤ç»­è·‘æœ€æ–° checkpoint

# ================= å¤šæ­¥ Unroll è®­ç»ƒé…ç½®(æ ¸å¿ƒ) =================
# çŸ­ unroll (2-4 æ­¥) è®©æ¨¡å‹åœ¨è®­ç»ƒæœŸå°±æš´éœ²åœ¨â€œè‡ªå›å½’è¯¯å·®â€ä¸‹ï¼Œä»æ ¹æºç¼“è§£é•¿æ—¶æ¼‚ç§»
UNROLL_STEPS = int(os.getenv('UNROLL_STEPS', '3'))  # 2-4 æ¨è
if UNROLL_STEPS < 1:
    raise ValueError("UNROLL_STEPS å¿…é¡» >= 1")

# å¤šæ­¥æŸå¤±ï¼šåŒæ—¶çº¦æŸçŠ¶æ€å’Œå€¼çš„å˜åŒ–é‡(å·®åˆ†)ï¼Œå‡å°‘â€œæ¼‚ç§»â€åŒæ—¶ä¿ç•™ç»†èŠ‚
STATE_LOSS_WEIGHT = float(os.getenv('STATE_LOSS_WEIGHT', '0.25'))  # ç»å¯¹åœº MSE æƒé‡
DELTA_LOSS_WEIGHT = float(os.getenv('DELTA_LOSS_WEIGHT', '1.0'))   # å·®åˆ† MSE æƒé‡
DELTA_SCALE = float(os.getenv('DELTA_SCALE', '100.0'))             # å·®åˆ†æ”¾å¤§å€æ•°(æ²¿ç”¨åŸç­–ç•¥)
STEP_WEIGHT_GAMMA = float(os.getenv('STEP_WEIGHT_GAMMA', '1.0'))   # æ¯æ­¥æƒé‡è¡°å‡(=1 è¡¨ç¤ºç­‰æƒ)

# ç‰©ç†é’³åˆ¶ï¼šè®­ç»ƒæœŸä¹ŸåŠ åŒé‡ä¿é™©ï¼Œé¿å…å¤šæ­¥åä¼ æ—¶æ•°å€¼ç‚¸æ‰
TRAIN_CLAMP = int(os.getenv('TRAIN_CLAMP', '1'))
CLAMP_MIN = float(os.getenv('CLAMP_MIN', '-0.5'))
CLAMP_MAX = float(os.getenv('CLAMP_MAX', '1.5'))
CLAMP_MODE = os.getenv('CLAMP_MODE', 'hard').lower()  # hard | hard_ste | smooth(softplus) | tanh | none
SMOOTH_CLAMP_BETA = float(os.getenv('SMOOTH_CLAMP_BETA', '200.0'))  # è¶Šå¤§è¶Šæ¥è¿‘ hard clampï¼Œä¸”åŒºé—´å†…æ›´æ¥è¿‘æ’ç­‰

# å¯é€‰ï¼šæ¢¯åº¦è£å‰ªï¼Œæå‡å¤šæ­¥åä¼ ç¨³å®šæ€§
CLIP_GRAD_NORM = float(os.getenv('CLIP_GRAD_NORM', '1.0'))  # è®¾ä¸º 0 å…³é—­

# æ˜¾å­˜ä¸å¤Ÿæ—¶ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§ batchï¼ˆä¸ä¼šå¢åŠ æ˜¾å­˜å ç”¨ï¼Œä»£ä»·æ˜¯æ›´æ…¢ï¼‰
GRAD_ACCUM_STEPS = int(os.getenv('GRAD_ACCUM_STEPS', '1'))

# ç‰©ç†ä¸€è‡´æ€§/ç»“æ„ä¸€è‡´æ€§æŸå¤±ï¼ˆæ›´é€‚åˆå†™è¿› POFï¼‰
# 1) Range penalty: è®©ç½‘ç»œå­¦ä¼šâ€œä¸è¦å»æ’ clampâ€ï¼Œå‡å°‘é•¿æœŸé¥±å’Œå¯¼è‡´çš„å½¢æ€å¤±çœŸ
#    soft clamp åœºæ™¯ä¸‹ä¸€èˆ¬ä¸éœ€è¦ï¼›é»˜è®¤å…³é—­ã€‚
RANGE_PENALTY_WEIGHT = float(os.getenv('RANGE_PENALTY_WEIGHT', '0.0'))
# 2) Vorticity loss: ç”¨åˆ‡ç‰‡æ¶¡é‡ä¿æŒæ¶¡ç»“æ„ï¼ˆÏ‰_x = âˆ‚W/âˆ‚y - âˆ‚V/âˆ‚zï¼Œå‡è®¾ H=Y, W=Zï¼‰
VORTICITY_LOSS_WEIGHT = float(os.getenv('VORTICITY_LOSS_WEIGHT', '0.05'))

def range_penalty(x, min_val, max_val):
    over = torch.relu(x - max_val)
    under = torch.relu(min_val - x)
    return (over * over + under * under).mean()

def soft_clamp_tanh(x, min_val, max_val):
    """
    ä¸æ¨èçš„â€œå‹ç¼©å¼â€è½¯é’³åˆ¶ï¼šæŠŠè¾“å‡ºæ•´ä½“å‹ç¼©æ˜ å°„åˆ° [min_val, max_val]ã€‚
    æ³¨æ„ï¼štanh æ˜ å°„ä¸æ˜¯åŒºé—´å†…æ’ç­‰ï¼Œå¯èƒ½å¯¼è‡´åŠ¨åŠ›å­¦è¢«â€œæ‹‰å¹³/å˜å½¢â€ï¼Œé•¿åºåˆ—æ›´æ˜æ˜¾ã€‚
    """
    mid = (max_val + min_val) * 0.5
    half = (max_val - min_val) * 0.5
    # half>0ï¼Œtanh å°†å®æ•°æ˜ å°„åˆ° (-1,1)
    return mid + half * torch.tanh((x - mid) / max(half, 1e-6))

def smooth_clamp_softplus(x, min_val, max_val, beta):
    """
    æ¨èçš„â€œæ’ç­‰å¼â€è½¯é’³åˆ¶ï¼šåŒºé—´å†…è¿‘ä¼¼æ’ç­‰ï¼Œè¶Šç•Œåå¹³æ»‘é¥±å’Œåˆ°è¾¹ç•Œã€‚
    y = x + softplus(min-x) - softplus(x-max)
    """
    beta = max(float(beta), 1e-6)
    return x + F.softplus(min_val - x, beta=beta, threshold=20.0) - F.softplus(x - max_val, beta=beta, threshold=20.0)

def hard_clamp_ste(x, min_val, max_val):
    """
    Hard clamp çš„â€œç›´é€šä¼°è®¡å™¨â€(STE)ï¼šå‰å‘ç­‰ä»·äº clampï¼Œåå‘æŠŠæ¢¯åº¦å½“æˆæ’ç­‰ä¼ é€’ï¼Œé¿å…è¶Šç•Œæ—¶æ¢¯åº¦é¥±å’Œã€‚
    """
    y = torch.clamp(x, min=min_val, max=max_val)
    return x + (y - x).detach()

def vorticity_x(field):
    """
    è®¡ç®—åˆ‡ç‰‡æ¶¡é‡ Ï‰_x = âˆ‚W/âˆ‚y - âˆ‚V/âˆ‚z
    field: (B,3,H,W) é€šé“é¡ºåº [U,V,W]
    """
    v = field[:, 1:2]
    w = field[:, 2:3]

    # ä¸­å¿ƒå·®åˆ†æ ¸
    ky = torch.tensor([[-0.5], [0.0], [0.5]], device=field.device, dtype=field.dtype).view(1, 1, 3, 1)
    kz = torch.tensor([[-0.5, 0.0, 0.5]], device=field.device, dtype=field.dtype).view(1, 1, 1, 3)

    dw_dy = F.conv2d(w, ky, padding=(1, 0))
    dv_dz = F.conv2d(v, kz, padding=(0, 1))
    return dw_dy - dv_dz

# æ£€æŸ¥æ˜¯å¦æœ‰GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"ğŸ”¥ æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# ================= 1. å®šä¹‰æ•°æ®é›† =================
class CFDDataset(Dataset):
    def __init__(self, data_dir, unroll_steps):
        """
        åŠ è½½æ‰€æœ‰ .npy æ•°æ®æ–‡ä»¶
        ä½¿ç”¨å†…å­˜æ˜ å°„æ–¹å¼é¿å…OOM
        """
        if unroll_steps < 1:
            raise ValueError("unroll_steps å¿…é¡» >= 1")
        self.unroll_steps = int(unroll_steps)

        # è·å–æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆæŒ‰æ•°å­—æ’åºï¼‰
        self.files = sorted(glob.glob(os.path.join(data_dir, '*.npy')))

        if len(self.files) == 0:
            raise ValueError(f"åœ¨ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° .npy æ–‡ä»¶ï¼")

        self.data_maps = []
        self.valid_indices = []

        print(f"ğŸ“‚ æ­£åœ¨å»ºç«‹æ•°æ®ç´¢å¼• (ä½¿ç”¨å†…å­˜æ˜ å°„ï¼Œä¸ä¼šçˆ†å†…å­˜)...")

        for file_idx, f in enumerate(self.files):
            # mmap_mode='r' å…³é”®ï¼åªå»ºç«‹æ˜ å°„ä¸è¯»å…¥å†…å­˜
            try:
                data = np.load(f, mmap_mode='r')
                self.data_maps.append(data)
                num_frames = data.shape[0]

                # ç”Ÿæˆæ ·æœ¬ç´¢å¼•: è¾“å…¥t -> é¢„æµ‹ t+1..t+K
                # éœ€è¦ä¿è¯æœ‰è¶³å¤Ÿçš„æœªæ¥å¸§ï¼Œå› æ­¤æœ€å K å¸§ä¸å‚ä¸è®­ç»ƒæ ·æœ¬
                for t in range(num_frames - self.unroll_steps):
                    self.valid_indices.append((file_idx, t))

                print(f"  âœ“ {os.path.basename(f)}: {data.shape}, {num_frames-self.unroll_steps} ä¸ªè®­ç»ƒæ ·æœ¬ (unroll={self.unroll_steps})")

            except Exception as e:
                print(f"  âŒ åŠ è½½ {f} å¤±è´¥: {e}")

        print(f"âœ… æ•°æ®é›†å°±ç»ªï¼å…± {len(self.files)} ä¸ªåˆ‡ç‰‡ï¼ŒåŒ…å« {len(self.valid_indices)} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        file_id, t = self.valid_indices[idx]

        # è¯»å–å½“å‰å¸§å’Œæœªæ¥ K å¸§
        # æ•°æ®æ ¼å¼: (H, W, C) = (200, 128, 3)
        current_frame = self.data_maps[file_id][t].astype(np.float32)
        future_frames = [
            self.data_maps[file_id][t + i].astype(np.float32)
            for i in range(1, self.unroll_steps + 1)
        ]

        # å½’ä¸€åŒ–ç­–ç•¥ï¼š
        # Xæ–¹å‘é€Ÿåº¦çº¦27ï¼Œæˆ‘ä»¬é™¤ä»¥ 30 è®©å®ƒå½’ä¸€åŒ–åˆ° 0-1 ä¹‹é—´
        # è¿™æ ·æ¨¡å‹æ”¶æ•›æ›´å¿«
        norm_factor = 30.0
        current_frame = current_frame / norm_factor
        future_frames = [f / norm_factor for f in future_frames]

        # è½¬ä¸º PyTorch Tensor: (H, W, C) -> (C, H, W)
        input_tensor = torch.from_numpy(current_frame).permute(2, 0, 1)
        target_tensors = torch.stack(
            [torch.from_numpy(f).permute(2, 0, 1) for f in future_frames],
            dim=0,
        )  # (K, C, H, W)

        return input_tensor, target_tensors

# ================= 2. å®šä¹‰æ¨¡å‹ (ResNetæ¶æ„) =================
class ResidualBlock(nn.Module):
    """æ®‹å·®å—ï¼šå­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„å˜åŒ–é‡"""
    def __init__(self, in_channels):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),  # åŠ å…¥BNå±‚åŠ é€Ÿæ”¶æ•›
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.conv(x)  # å­¦ä¹ æ®‹å·®

class CFDPredictor(nn.Module):
    """CFD é€Ÿåº¦åœºé¢„æµ‹å™¨ - ResNet æ¶æ„"""
    def __init__(self):
        super(CFDPredictor, self).__init__()

        # ç¼–ç å™¨ (ä¸‹é‡‡æ ·ï¼Œæå–ç‰¹å¾)
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),  # 128x200 -> 64x100
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),  # 64x100 -> 32x50
            nn.ReLU(inplace=True)
        )

        # ç“¶é¢ˆå±‚ (å­¦ä¹ ç‰©ç†è§„å¾‹)
        self.bottleneck = nn.Sequential(
            ResidualBlock(256),
            ResidualBlock(256),
            ResidualBlock(256),
            ResidualBlock(256)
        )

        # è§£ç å™¨ (ä¸Šé‡‡æ ·ï¼Œæ¢å¤åˆ†è¾¨ç‡)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 32x50 -> 64x100
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64x100 -> 128x200
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)  # è¾“å‡ºå±‚
        )

    def forward(self, x):
        # ç¼–ç 
        features = self.encoder(x)

        # å­¦ä¹ ç‰©ç†è§„å¾‹
        features = self.bottleneck(features)

        # è§£ç å¾—åˆ°å˜åŒ–é‡
        delta = self.decoder(features)

        # ç‰©ç†çº¦æŸï¼šä¸‹ä¸€æ—¶åˆ» = å½“å‰æ—¶åˆ» + å˜åŒ–é‡
        return x + delta

# ================= 3. æ£€æŸ¥ç‚¹ç®¡ç† =================
def save_checkpoint(model, optimizer, epoch, loss, filename):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'timestamp': datetime.now().isoformat()
    }

    filepath = os.path.join(SAVE_DIR, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")

def load_checkpoint(model, optimizer, filepath):
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
    if not os.path.exists(filepath):
        return 0

    checkpoint = torch.load(filepath, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1

    print(f"ğŸ“¥ å·²åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
    print(f"   ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒ")

    return start_epoch

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoints = glob.glob(os.path.join(SAVE_DIR, 'resnet_epoch_*.pth'))
    if not checkpoints:
        return None

    # æŒ‰ epoch æ•°å­—æ’åº
    checkpoints.sort(key=lambda x: int(x.split('_')[-1].replace('.pth', '')))
    return checkpoints[-1]

def get_resume_checkpoint():
    """
    è·å–ç”¨äºæ¢å¤è®­ç»ƒçš„ checkpoint:
    - é»˜è®¤ä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ checkpointï¼ˆæ”¯æŒå¾®è°ƒä¸­æ–­åç»§ç»­ï¼‰
    - ä¼˜å…ˆ resnet_interrupted.pthï¼ˆè‹¥æ¯”æœ€æ–° epoch checkpoint æ›´æ–°ï¼‰
    - è‹¥æœªå¯ç”¨ RESUME_LATESTï¼Œåˆ™ä¼˜å…ˆ FINETUNE_FROM
    """
    latest_epoch_ckpt = get_latest_checkpoint()
    interrupted = os.path.join(SAVE_DIR, 'resnet_interrupted.pth')
    if RESUME_LATEST:
        if os.path.exists(interrupted):
            if (not latest_epoch_ckpt) or (not os.path.exists(latest_epoch_ckpt)):
                return interrupted
            if os.path.getmtime(interrupted) >= os.path.getmtime(latest_epoch_ckpt):
                return interrupted
        if latest_epoch_ckpt and os.path.exists(latest_epoch_ckpt):
            return latest_epoch_ckpt

    if os.path.exists(FINETUNE_FROM):
        return FINETUNE_FROM
    if latest_epoch_ckpt and os.path.exists(latest_epoch_ckpt):
        return latest_epoch_ckpt
    if os.path.exists(interrupted):
        return interrupted

    return None

def get_checkpoint_start_epoch(filepath):
    """
    è¯»å– checkpoint çš„èµ·å§‹ epochï¼ˆ= ckpt['epoch'] + 1ï¼‰ã€‚
    è‹¥ä¸æ˜¯åŒ…å«å…ƒæ•°æ®çš„ checkpointï¼ˆä»…æƒé‡ï¼‰ï¼Œè¿”å› Noneã€‚
    """
    ckpt = torch.load(filepath, map_location='cpu')
    if isinstance(ckpt, dict) and 'epoch' in ckpt:
        return int(ckpt['epoch']) + 1
    return None

# ================= 4. è®­ç»ƒä¸»ç¨‹åº =================
if __name__ == '__main__':
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    print("="*60)
    print("CFD æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("="*60)

    # 1. å‡†å¤‡æ•°æ®
    print("\n[1/4] å‡†å¤‡æ•°æ®é›†...")
    dataset = CFDDataset(DATA_DIR, unroll_steps=UNROLL_STEPS)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True  # åŠ é€Ÿ CPU->GPU ä¼ è¾“
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("\n[2/4] åˆå§‹åŒ–æ¨¡å‹...")
    model = CFDPredictor().to(device)

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")

    # 3. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±

    # 4. æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹å¯ä»¥æ¢å¤
    print("\n[3/4] æ£€æŸ¥æ–­ç‚¹...")
    start_epoch = 0
    resume_ckpt = get_resume_checkpoint()
    if resume_ckpt:
        start_epoch = load_checkpoint(model, optimizer, resume_ckpt)
    else:
        raise RuntimeError(
            f"æœªæ‰¾åˆ°å¯ç”¨ checkpointï¼ˆFINETUNE_FROM={FINETUNE_FROM}ï¼Œä¸” {SAVE_DIR}/resnet_epoch_*.pth ä¸å­˜åœ¨ï¼‰ã€‚"
            "æŒ‰äº¤æ¥è¦æ±‚ä¸è¦ä»å¤´è®­ç»ƒï¼Œè¯·å…ˆç¡®è®¤æƒé‡æ–‡ä»¶åœ¨ checkpoints/ ä¸‹ã€‚"
        )

    # 5. å¼€å§‹è®­ç»ƒ
    print(f"\n[4/4] å¼€å§‹è®­ç»ƒï¼")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   å­¦ä¹ ç‡: {LEARNING_RATE}")
    finetune_origin_ckpt = FINETUNE_FROM if os.path.exists(FINETUNE_FROM) else resume_ckpt
    finetune_origin_start_epoch = get_checkpoint_start_epoch(finetune_origin_ckpt) or start_epoch
    target_end_epoch = finetune_origin_start_epoch + EXTRA_EPOCHS
    end_epoch = target_end_epoch
    if start_epoch >= end_epoch:
        print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡è½®æ•°ï¼šå½“å‰ start_epoch={start_epoch}ï¼Œç›®æ ‡ end_epoch={end_epoch}ï¼Œæ— éœ€ç»§ç»­è®­ç»ƒã€‚")
        raise SystemExit(0)

    print(f"   è®­ç»ƒè½®æ•°: {end_epoch} (èµ·å§‹ {start_epoch}, ç›®æ ‡ç»“æŸ {end_epoch})")
    print(f"   èµ·å§‹ Epoch: {start_epoch}")
    print(f"   æ¢å¤è®­ç»ƒ: {resume_ckpt}")
    print(f"   å¾®è°ƒåŸºå‡†: {finetune_origin_ckpt} (origin_start={finetune_origin_start_epoch}, extra={EXTRA_EPOCHS})")
    print(f"   Unroll: steps={UNROLL_STEPS}, gamma={STEP_WEIGHT_GAMMA}")
    print(f"   Loss: state_w={STATE_LOSS_WEIGHT}, delta_w={DELTA_LOSS_WEIGHT}, delta_scale={DELTA_SCALE}")
    print(f"   Clamp: enable={TRAIN_CLAMP}, mode={CLAMP_MODE}, min={CLAMP_MIN}, max={CLAMP_MAX}, smooth_beta={SMOOTH_CLAMP_BETA}")
    print(f"   GradClip: max_norm={CLIP_GRAD_NORM}")
    print("="*60)

    try:
        avg_loss = 0.0
        for epoch in range(start_epoch, end_epoch):
            model.train()
            total_loss = 0

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{end_epoch}")

            for batch_idx, (inputs, targets_seq) in enumerate(pbar):
                # æ•°æ®ç§»åˆ° GPU
                inputs = inputs.to(device)
                targets_seq = targets_seq.to(device)  # (B, K, C, H, W)

                # å‰å‘ä¼ æ’­
                if batch_idx % GRAD_ACCUM_STEPS == 0:
                    optimizer.zero_grad(set_to_none=True)
                loss = 0.0

                pred_prev = inputs
                gt_prev = inputs

                steps = min(UNROLL_STEPS, targets_seq.shape[1])
                for s in range(steps):
                    pred_raw = model(pred_prev)

                    if TRAIN_CLAMP and CLAMP_MODE != 'none':
                        if CLAMP_MODE in ('smooth', 'softplus'):
                            pred = smooth_clamp_softplus(pred_raw, CLAMP_MIN, CLAMP_MAX, SMOOTH_CLAMP_BETA)
                        elif CLAMP_MODE in ('soft', 'tanh'):
                            pred = soft_clamp_tanh(pred_raw, CLAMP_MIN, CLAMP_MAX)
                        elif CLAMP_MODE == 'hard':
                            pred = torch.clamp(pred_raw, min=CLAMP_MIN, max=CLAMP_MAX)
                        elif CLAMP_MODE in ('hard_ste', 'ste'):
                            pred = hard_clamp_ste(pred_raw, CLAMP_MIN, CLAMP_MAX)
                        else:
                            raise ValueError(f"æœªçŸ¥ CLAMP_MODE: {CLAMP_MODE} (æœŸæœ› hard|hard_ste|smooth|tanh|none)")
                    else:
                        pred = pred_raw

                    gt = targets_seq[:, s]
                    step_w = STEP_WEIGHT_GAMMA ** s

                    if STATE_LOSS_WEIGHT > 0:
                        loss = loss + step_w * STATE_LOSS_WEIGHT * criterion(pred, gt)

                    if DELTA_LOSS_WEIGHT > 0:
                        delta_pred = pred - pred_prev
                        delta_gt = gt - gt_prev
                        loss = loss + step_w * DELTA_LOSS_WEIGHT * criterion(delta_pred * DELTA_SCALE, delta_gt * DELTA_SCALE)

                    if VORTICITY_LOSS_WEIGHT > 0:
                        vort_pred = vorticity_x(pred)
                        vort_gt = vorticity_x(gt)
                        loss = loss + step_w * VORTICITY_LOSS_WEIGHT * criterion(vort_pred, vort_gt)

                    if TRAIN_CLAMP and CLAMP_MODE == 'hard' and RANGE_PENALTY_WEIGHT > 0:
                        loss = loss + step_w * RANGE_PENALTY_WEIGHT * range_penalty(pred_raw, CLAMP_MIN, CLAMP_MAX)

                    pred_prev = pred
                    gt_prev = gt

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                (loss / GRAD_ACCUM_STEPS).backward()
                is_step = ((batch_idx + 1) % GRAD_ACCUM_STEPS == 0) or (batch_idx + 1 == len(dataloader))
                if is_step:
                    if CLIP_GRAD_NORM and CLIP_GRAD_NORM > 0:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)
                    optimizer.step()

                # ç»Ÿè®¡
                total_loss += loss.item()
                pbar.set_postfix({'Loss': f"{loss.item():.6f}"})

            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / len(dataloader)
            print(f"\nEpoch {epoch+1}/{end_epoch} å®Œæˆ | å¹³å‡ Loss: {avg_loss:.8f}")

            # æ¯10è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                save_checkpoint(
                    model,
                    optimizer,
                    epoch,
                    avg_loss,
                    f"resnet_epoch_{epoch+1}.pth"
                )

        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_path = os.path.join(SAVE_DIR, "resnet_unroll_softclamp_final.pth")
        torch.save(model.state_dict(), final_path)
        print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        print(f"ğŸ“ æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹: {SAVE_DIR}")

    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        save_checkpoint(model, optimizer, epoch, avg_loss, "resnet_interrupted.pth")
        print("ğŸ’¾ å·²ä¿å­˜ä¸­æ–­æ—¶çš„æ£€æŸ¥ç‚¹")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        save_checkpoint(model, optimizer, epoch, 0, "resnet_error.pth")
        print("ğŸ’¾ å·²ä¿å­˜é”™è¯¯æ—¶çš„æ£€æŸ¥ç‚¹")
        raise
