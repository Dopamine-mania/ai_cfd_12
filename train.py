import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
from datetime import datetime

# ================= æ ¸å¿ƒé…ç½®åŒºåŸŸ =================
BATCH_SIZE = 16          # A40æ˜¾å­˜å¾ˆå¤§ï¼Œ16æ²¡é—®é¢˜ï¼Œå¦‚æœçˆ†æ˜¾å­˜æ”¹æˆ8
LEARNING_RATE = 1e-5     # å­¦ä¹ ç‡
EPOCHS = 100             # è®­ç»ƒè½®æ•°
DATA_DIR = './processed_data'  # æ•°æ®è·¯å¾„
SAVE_DIR = './checkpoints'     # æ¨¡å‹ä¿å­˜è·¯å¾„

# æ£€æŸ¥æ˜¯å¦æœ‰GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"ğŸ”¥ æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# ================= 1. å®šä¹‰æ•°æ®é›† =================
class CFDDataset(Dataset):
    def __init__(self, data_dir):
        """
        åŠ è½½æ‰€æœ‰ .npy æ•°æ®æ–‡ä»¶
        ä½¿ç”¨å†…å­˜æ˜ å°„æ–¹å¼é¿å…OOM
        """
        # è·å–æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆæŒ‰æ•°å­—æ’åºï¼‰
        self.files = sorted(glob.glob(os.path.join(data_dir, '*.npy')))

        if len(self.files) == 0:
            raise ValueError(f"åœ¨ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° .npy æ–‡ä»¶ï¼")

        self.data_maps = []
        self.valid_indices = []

        print(f"ğŸ“‚ æ­£åœ¨å»ºç«‹æ•°æ®ç´¢å¼• (ä½¿ç”¨å†…å­˜æ˜ å°„ï¼Œä¸ä¼šçˆ†å†…å­˜)...")

        for file_idx, f in enumerate(self.files):
            # mmap_mode='r' å…³é”®ï¼åªå»ºç«‹æ˜ å°„ä¸è¯»å…¥å†…å­˜
            try:
                data = np.load(f, mmap_mode='r')
                self.data_maps.append(data)
                num_frames = data.shape[0]

                # ç”Ÿæˆæ ·æœ¬ç´¢å¼•: è¾“å…¥t -> é¢„æµ‹t+1
                # æœ€åä¸€å¸§æ²¡æœ‰åç»­å¸§ï¼Œæ‰€ä»¥ä¸åŒ…å«åœ¨è®­ç»ƒé›†ä¸­
                for t in range(num_frames - 1):
                    self.valid_indices.append((file_idx, t))

                print(f"  âœ“ {os.path.basename(f)}: {data.shape}, {num_frames-1} ä¸ªè®­ç»ƒæ ·æœ¬")

            except Exception as e:
                print(f"  âŒ åŠ è½½ {f} å¤±è´¥: {e}")

        print(f"âœ… æ•°æ®é›†å°±ç»ªï¼å…± {len(self.files)} ä¸ªåˆ‡ç‰‡ï¼ŒåŒ…å« {len(self.valid_indices)} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        file_id, t = self.valid_indices[idx]

        # è¯»å–å½“å‰å¸§å’Œä¸‹ä¸€å¸§
        # æ•°æ®æ ¼å¼: (H, W, C) = (128, 200, 3)
        current_frame = self.data_maps[file_id][t].astype(np.float32)
        next_frame = self.data_maps[file_id][t+1].astype(np.float32)

        # å½’ä¸€åŒ–ç­–ç•¥ï¼š
        # Xæ–¹å‘é€Ÿåº¦çº¦27ï¼Œæˆ‘ä»¬é™¤ä»¥ 30 è®©å®ƒå½’ä¸€åŒ–åˆ° 0-1 ä¹‹é—´
        # è¿™æ ·æ¨¡å‹æ”¶æ•›æ›´å¿«
        norm_factor = 30.0
        current_frame = current_frame / norm_factor
        next_frame = next_frame / norm_factor

        # è½¬ä¸º PyTorch Tensor: (H, W, C) -> (C, H, W)
        input_tensor = torch.from_numpy(current_frame).permute(2, 0, 1)
        target_tensor = torch.from_numpy(next_frame).permute(2, 0, 1)

        return input_tensor, target_tensor

# ================= 2. å®šä¹‰æ¨¡å‹ (ResNetæ¶æ„) =================
class ResidualBlock(nn.Module):
    """æ®‹å·®å—ï¼šå­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„å˜åŒ–é‡"""
    def __init__(self, in_channels):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels),  # åŠ å…¥BNå±‚åŠ é€Ÿæ”¶æ•›
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.conv(x)  # å­¦ä¹ æ®‹å·®

class CFDPredictor(nn.Module):
    """CFD é€Ÿåº¦åœºé¢„æµ‹å™¨ - ResNet æ¶æ„"""
    def __init__(self):
        super(CFDPredictor, self).__init__()

        # ç¼–ç å™¨ (ä¸‹é‡‡æ ·ï¼Œæå–ç‰¹å¾)
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),  # 128x200 -> 64x100
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),  # 64x100 -> 32x50
            nn.ReLU(inplace=True)
        )

        # ç“¶é¢ˆå±‚ (å­¦ä¹ ç‰©ç†è§„å¾‹)
        self.bottleneck = nn.Sequential(
            ResidualBlock(256),
            ResidualBlock(256),
            ResidualBlock(256),
            ResidualBlock(256)
        )

        # è§£ç å™¨ (ä¸Šé‡‡æ ·ï¼Œæ¢å¤åˆ†è¾¨ç‡)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 32x50 -> 64x100
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64x100 -> 128x200
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)  # è¾“å‡ºå±‚
        )

    def forward(self, x):
        # ç¼–ç 
        features = self.encoder(x)

        # å­¦ä¹ ç‰©ç†è§„å¾‹
        features = self.bottleneck(features)

        # è§£ç å¾—åˆ°å˜åŒ–é‡
        delta = self.decoder(features)

        # ç‰©ç†çº¦æŸï¼šä¸‹ä¸€æ—¶åˆ» = å½“å‰æ—¶åˆ» + å˜åŒ–é‡
        return x + delta

# ================= 3. æ£€æŸ¥ç‚¹ç®¡ç† =================
def save_checkpoint(model, optimizer, epoch, loss, filename):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'timestamp': datetime.now().isoformat()
    }

    filepath = os.path.join(SAVE_DIR, filename)
    torch.save(checkpoint, filepath)
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")

def load_checkpoint(model, optimizer, filepath):
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
    if not os.path.exists(filepath):
        return 0

    checkpoint = torch.load(filepath)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1

    print(f"ğŸ“¥ å·²åŠ è½½æ£€æŸ¥ç‚¹: {filepath}")
    print(f"   ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒ")

    return start_epoch

def get_latest_checkpoint():
    """è·å–æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoints = glob.glob(os.path.join(SAVE_DIR, 'resnet_epoch_*.pth'))
    if not checkpoints:
        return None

    # æŒ‰ epoch æ•°å­—æ’åº
    checkpoints.sort(key=lambda x: int(x.split('_')[-1].replace('.pth', '')))
    return checkpoints[-1]

# ================= 4. è®­ç»ƒä¸»ç¨‹åº =================
if __name__ == '__main__':
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    print("="*60)
    print("CFD æ·±åº¦å­¦ä¹ è®­ç»ƒç³»ç»Ÿ")
    print("="*60)

    # 1. å‡†å¤‡æ•°æ®
    print("\n[1/4] å‡†å¤‡æ•°æ®é›†...")
    dataset = CFDDataset(DATA_DIR)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True  # åŠ é€Ÿ CPU->GPU ä¼ è¾“
    )

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("\n[2/4] åˆå§‹åŒ–æ¨¡å‹...")
    model = CFDPredictor().to(device)

    # åŠ è½½ä¹‹å‰çš„æƒé‡ç»§ç»­è·‘
    checkpoint_path = './checkpoints/resnet_epoch_10.pth'
    if os.path.exists(checkpoint_path):
         model.load_state_dict(torch.load(checkpoint_path)['model_state_dict'])
         print("âœ… å·²åŠ è½½ Epoch 10 æƒé‡ï¼Œç»§ç»­è®­ç»ƒ...")

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")

    # 3. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±

    # 4. æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹å¯ä»¥æ¢å¤
    print("\n[3/4] æ£€æŸ¥æ–­ç‚¹...")
    latest_checkpoint = get_latest_checkpoint()
    if latest_checkpoint:
        start_epoch = load_checkpoint(model, optimizer, latest_checkpoint)
    else:
        start_epoch = 0
        print("   æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # 5. å¼€å§‹è®­ç»ƒ
    print(f"\n[4/4] å¼€å§‹è®­ç»ƒï¼")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"   è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"   èµ·å§‹ Epoch: {start_epoch}")
    print("="*60)

    try:
        for epoch in range(start_epoch, EPOCHS):
            model.train()
            total_loss = 0

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")

            for batch_idx, (inputs, targets) in enumerate(pbar):
                # æ•°æ®ç§»åˆ° GPU
                inputs, targets = inputs.to(device), targets.to(device)

                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                outputs = model(inputs)

                # è®¡ç®—æŸå¤± - æ”¹è¿›ç‰ˆï¼šå…³æ³¨ç»†èŠ‚å˜åŒ–
                # 1. ç®—å‡ºæ¨¡å‹é¢„æµ‹çš„"å˜åŒ–é‡" (Prediction Delta)
                diff_pred = outputs - inputs

                # 2. ç®—å‡ºçœŸå®çš„"å˜åŒ–é‡" (Ground Truth Delta)
                diff_gt = targets - inputs

                # 3. æŠŠç»†èŠ‚æ”¾å¤§ 100 å€å†ç®— Lossï¼
                # è¿™æ ·æ¨¡å‹å°±å¿…é¡»å…³æ³¨é‚£äº›å¾®å°çš„æ¶¡æµå˜åŒ–ï¼Œå¦åˆ™ Loss ä¼šå¾ˆå¤§
                loss = criterion(diff_pred * 100.0, diff_gt * 100.0)

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                loss.backward()
                optimizer.step()

                # ç»Ÿè®¡
                total_loss += loss.item()
                pbar.set_postfix({'Loss': f"{loss.item():.6f}"})

            # è®¡ç®—å¹³å‡æŸå¤±
            avg_loss = total_loss / len(dataloader)
            print(f"\nEpoch {epoch+1}/{EPOCHS} å®Œæˆ | å¹³å‡ Loss: {avg_loss:.8f}")

            # æ¯10è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                save_checkpoint(
                    model,
                    optimizer,
                    epoch,
                    avg_loss,
                    f"resnet_epoch_{epoch+1}.pth"
                )

        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_path = os.path.join(SAVE_DIR, "resnet_final.pth")
        torch.save(model.state_dict(), final_path)
        print(f"\nğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_path}")
        print(f"ğŸ“ æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹: {SAVE_DIR}")

    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        save_checkpoint(model, optimizer, epoch, avg_loss, "resnet_interrupted.pth")
        print("ğŸ’¾ å·²ä¿å­˜ä¸­æ–­æ—¶çš„æ£€æŸ¥ç‚¹")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        save_checkpoint(model, optimizer, epoch, 0, "resnet_error.pth")
        print("ğŸ’¾ å·²ä¿å­˜é”™è¯¯æ—¶çš„æ£€æŸ¥ç‚¹")
        raise
